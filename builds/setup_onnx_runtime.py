#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ONNXRuntime Setup for PyInstaller
ONNXRuntimeã®ä¾å­˜é–¢ä¿‚ã‚’ç¢ºå®Ÿã«è§£æ±ºã™ã‚‹ãŸã‚ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ï¼ˆCUDAç’°å¢ƒè‡ªå‹•æ¤œå‡ºå¯¾å¿œï¼‰
"""

import os
import sys
import shutil
import subprocess
from pathlib import Path

def check_cuda_availability():
    """CUDAç’°å¢ƒã®åˆ©ç”¨å¯èƒ½æ€§ã‚’è©³ç´°ãƒã‚§ãƒƒã‚¯"""
    cuda_info = {
        "pytorch_available": False,
        "cuda_available": False,
        "onnx_gpu_compatible": False,
        "device_count": 0,
        "device_name": "Unknown",
        "cuda_version": "Unknown",
        "cudnn_available": False,
        "errors": []
    }
    
    try:
        import torch
        cuda_info["pytorch_available"] = True
        cuda_info["cuda_version"] = getattr(torch.version, 'cuda', 'Unknown')
        
        print(f"âœ… PyTorchæ¤œå‡º: {torch.__version__} (CUDA: {cuda_info['cuda_version']})")
        
        # CUDAåŸºæœ¬ãƒã‚§ãƒƒã‚¯
        if torch.cuda.is_available():
            cuda_info["cuda_available"] = True
            cuda_info["device_count"] = torch.cuda.device_count()
            if cuda_info["device_count"] > 0:
                cuda_info["device_name"] = torch.cuda.get_device_name(0)
                print(f"âœ… CUDAç’°å¢ƒæ¤œå‡º: {cuda_info['device_name']} ({cuda_info['device_count']} devices)")
                
                # ãƒ¡ãƒ¢ãƒªãƒã‚§ãƒƒã‚¯
                try:
                    memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    print(f"ðŸ“Š GPU Memory: {memory_total:.1f}GB")
                    
                    # ç°¡å˜ãªæŽ¨è«–ãƒ†ã‚¹ãƒˆ
                    test_tensor = torch.randn(1, 3, 224, 224).cuda()
                    _ = test_tensor.sum()
                    print("âœ… CUDAæŽ¨è«–ãƒ†ã‚¹ãƒˆæˆåŠŸ")
                    
                    # cuDNNãƒã‚§ãƒƒã‚¯
                    if torch.backends.cudnn.is_available():
                        cuda_info["cudnn_available"] = True
                        print(f"âœ… cuDNNåˆ©ç”¨å¯èƒ½: v{torch.backends.cudnn.version()}")
                    else:
                        print("âš ï¸ cuDNNåˆ©ç”¨ä¸å¯")
                        
                except Exception as e:
                    print(f"âš ï¸ CUDAå‹•ä½œãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
                    cuda_info["errors"].append(f"CUDA operation test failed: {e}")
            else:
                print("âš ï¸ CUDAãƒ‡ãƒã‚¤ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                cuda_info["errors"].append("No CUDA devices found")
        else:
            print("âš ï¸ torch.cuda.is_available() = False")
            cuda_info["errors"].append("torch.cuda.is_available() returned False")
            
    except ImportError:
        print("âŒ PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
        cuda_info["errors"].append("PyTorch not installed")
    except Exception as e:
        print(f"âŒ CUDAç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        cuda_info["errors"].append(f"CUDA check error: {e}")
    
    # ONNXRuntime-GPUäº’æ›æ€§åˆ¤å®š
    if cuda_info["cuda_available"] and cuda_info["cudnn_available"]:
        cuda_info["onnx_gpu_compatible"] = True
        print("ðŸš€ ONNXRuntime-GPUäº’æ›ç’°å¢ƒã‚’æ¤œå‡º")
        return True
    else:
        print("ðŸ’» CPUç’°å¢ƒã¨ã—ã¦å‹•ä½œã—ã¾ã™")
        print("ðŸ“ GPUç‰ˆã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ä»¥ä¸‹ãŒå¿…è¦ã§ã™:")
        print("   - CUDA Toolkit (11.8ä»¥ä¸Š)")
        print("   - cuDNN (8.0ä»¥ä¸Š)")
        print("   - äº’æ›æ€§ã®ã‚ã‚‹NVIDIA GPU")
        return False

def setup_onnxruntime_for_exe():
    """ONNXRuntimeã‚’exeåŒ–ã«é©ã—ãŸçŠ¶æ…‹ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆCUDAå¯¾å¿œï¼‰"""
    
    print("ðŸ”§ ONNXRuntime exeåŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’é–‹å§‹...")
    
    # CUDAç’°å¢ƒãƒã‚§ãƒƒã‚¯
    cuda_available = check_cuda_availability()
    
    try:
        # 1. äº’æ›æ€§ã®ã‚ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚»ãƒƒãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        print("ðŸ“¦ äº’æ›æ€§ã®ã‚ã‚‹ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚»ãƒƒãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
        
        # æ—¢å­˜ã®ONNXé–¢é€£ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’å®Œå…¨å‰Šé™¤
        print("ðŸ“¦ æ—¢å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¢ãƒ³ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...")
        subprocess.run([
            sys.executable, "-m", "pip", "uninstall", 
            "onnxruntime", "onnxruntime-gpu", "onnx", "numpy", "-y"
        ], check=False)  # ã‚¨ãƒ©ãƒ¼ã‚’è¡¨ç¤ºã—ã¦ç¢ºèª
        
        # CUDAç’°å¢ƒã«å¿œã˜ã¦ONNXRuntimeãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’é¸æŠžï¼ˆå®‰å…¨ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
        if cuda_available:
            print("ðŸš€ CUDAç’°å¢ƒæ¤œå‡º - GPUç‰ˆONNXRuntimeã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            onnxruntime_package = "onnxruntime-gpu==1.22.0"
            fallback_package = "onnxruntime==1.22.0"  # GPUç‰ˆå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        else:
            print("ðŸ’» CPUç’°å¢ƒ - CPUç‰ˆONNXRuntimeã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
            onnxruntime_package = "onnxruntime==1.22.0"
            fallback_package = None
        
        # æœ€æ–°ã®äº’æ›æ€§ã®ã‚ã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚»ãƒƒãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (2024å¹´12æœˆæ›´æ–°)
        compatible_packages = [
            "numpy>=2.3.0,<3.0.0",   # ONNXRuntime 1.22.0ã¨äº’æ›æ€§ã®ã‚ã‚‹NumPyï¼ˆæœ€æ–°ç‰ˆå¯¾å¿œï¼‰
            "onnx==1.16.0",           # ONNXRuntime 1.22.0ã¨äº’æ›æ€§ã®ã‚ã‚‹ONNX (IR v10ã‚µãƒãƒ¼ãƒˆ)
            onnxruntime_package       # ç’°å¢ƒã«å¿œã˜ãŸONNXRuntime
        ]
        
        for package in compatible_packages:
            print(f"ðŸ“¦ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­: {package}")
            try:
                if "numpy" in package:
                    # NumPyã¯æœ€æ–°ç‰ˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰
                    subprocess.run([
                        sys.executable, "-m", "pip", "install", package, "--upgrade", "--no-cache-dir"
                    ], check=True, capture_output=True)
                elif "onnxruntime-gpu" in package:
                    # GPUç‰ˆã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆå¤±æ•—æ™‚ã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    try:
                        subprocess.run([
                            sys.executable, "-m", "pip", "install", package, "--no-deps", "--disable-pip-version-check"
                        ], check=True, capture_output=True)
                        print("âœ… ONNXRuntime-GPU ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸ")
                    except subprocess.CalledProcessError as e:
                        print(f"âš ï¸ ONNXRuntime-GPU ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {e}")
                        if fallback_package:
                            print(f"ðŸ”„ CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {fallback_package}")
                            subprocess.run([
                                sys.executable, "-m", "pip", "install", fallback_package, "--no-deps"
                            ], check=True)
                            print("âœ… ONNXRuntime-CPU ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ")
                        else:
                            raise
                else:
                    subprocess.run([
                        sys.executable, "-m", "pip", "install", package, "--no-deps"
                    ], check=True)
            except subprocess.CalledProcessError as e:
                print(f"âŒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—: {package} - {e}")
                if "onnxruntime" not in package:  # ONNXRuntimeä»¥å¤–ã®ã‚¨ãƒ©ãƒ¼ã¯ç¶šè¡Œ
                    continue
                else:
                    raise
        
        # NudeNetã‚’å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ä¾å­˜é–¢ä¿‚ã‚’è§£æ±º
        print("ðŸ“¦ NudeNetä¾å­˜é–¢ä¿‚ä¿®æ­£ä¸­...")
        subprocess.run([
            sys.executable, "-m", "pip", "install", "nudenet", "--force-reinstall", "--no-deps"
        ], check=False)  # ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦NudeNetã‚’å¼·åˆ¶ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        
        # å¿…è¦ãªä¾å­˜é–¢ä¿‚ã‚’å€‹åˆ¥ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        dependencies = [
            "coloredlogs", "flatbuffers", "packaging", "protobuf", 
            "sympy", "humanfriendly", "pyreadline3", "mpmath", "typing_extensions"
        ]
        
        for dep in dependencies:
            subprocess.run([
                sys.executable, "-m", "pip", "install", dep
            ], check=False)  # ä¾å­˜é–¢ä¿‚ã¯å¤±æ•—ã—ã¦ã‚‚ç¶šè¡Œ
        
        print("âœ… äº’æ›æ€§ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
        
        # 2. ONNXRuntimeã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
        import onnxruntime as ort
        print(f"âœ… ONNXRuntime version: {ort.__version__}")
        print(f"âœ… ONNXRuntime path: {ort.__file__}")
        
        # 3. NumPyãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèªï¼ˆåŽ³æ ¼ãƒã‚§ãƒƒã‚¯ï¼‰
        import numpy as np
        numpy_version = np.__version__
        print(f"âœ… NumPy version: {numpy_version}")
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒã‚§ãƒƒã‚¯ï¼ˆ2.3.0ä»¥ä¸Šã‚’æŽ¨å¥¨ï¼‰
        major, minor = map(int, numpy_version.split('.')[:2])
        if major < 2 or (major == 2 and minor < 3):
            print(f"âš ï¸ è­¦å‘Š: å¤ã„NumPyãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆç¾åœ¨: {numpy_version}, æŽ¨å¥¨: 2.3.0ä»¥ä¸Šï¼‰")
            print("ðŸ”„ NumPyã‚’æœ€æ–°ç‰ˆã«ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ã¾ã™...")
            subprocess.run([
                sys.executable, "-m", "pip", "install", "numpy>=2.3.0,<3.0.0", "--upgrade", "--no-cache-dir"
            ], check=True)
            # å†ç¢ºèª
            import importlib
            importlib.reload(np)
            print(f"âœ… NumPyã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰å¾Œ: {np.__version__}")
        else:
            print(f"âœ… NumPy {numpy_version} ã¯æœ€æ–°è¦ä»¶ã‚’æº€ãŸã—ã¦ã„ã¾ã™")
        
        # 4. ONNXãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
        import onnx
        print(f"âœ… ONNX version: {onnx.__version__}")
        
        # 5. åˆ©ç”¨å¯èƒ½ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ç¢ºèª
        providers = ort.get_available_providers()
        print(f"âœ… Available providers: {providers}")
        
        # CUDAå¯¾å¿œç¢ºèª
        if "CUDAExecutionProvider" in providers:
            print("ðŸš€ CUDAExecutionProvideråˆ©ç”¨å¯èƒ½ - GPUåŠ é€Ÿæœ‰åŠ¹")
        else:
            print("ðŸ’» CPUExecutionProviderã®ã¿åˆ©ç”¨å¯èƒ½")
        
        # 6. ONNXRuntimeã®DLLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
        onnx_dir = Path(ort.__file__).parent
        dll_files = list(onnx_dir.glob("**/*.dll"))
        print(f"âœ… Found {len(dll_files)} DLL files:")
        for dll in dll_files:
            print(f"   - {dll}")
        
        # 7. äº’æ›æ€§ã®ã‚ã‚‹ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã§æ¤œè¨¼
        print("ðŸ§ª ONNXRuntimeäº’æ›æ€§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
        
        try:
            from onnx import helper, TensorProto
            
            # ONNX IR version 10 (ONNXRuntime 1.18.1äº’æ›) ã§ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            input_tensor = helper.make_tensor_value_info('input', TensorProto.FLOAT, [1, 3])
            output_tensor = helper.make_tensor_value_info('output', TensorProto.FLOAT, [1, 3])
            
            # æ’ç­‰å¤‰æ›ãƒŽãƒ¼ãƒ‰
            identity_node = helper.make_node('Identity', ['input'], ['output'])
            
            # ã‚°ãƒ©ãƒ•ä½œæˆ
            graph = helper.make_graph([identity_node], 'test_graph', [input_tensor], [output_tensor])
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆIR version 10ã‚’æ˜Žç¤ºçš„ã«æŒ‡å®šï¼‰
            model = helper.make_model(graph, ir_version=10)
            
            # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒã‚¤ãƒˆå½¢å¼ã«å¤‰æ›
            model_bytes = model.SerializeToString()
            
            # æ®µéšŽçš„ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ
            test_success = False
            
            if "CUDAExecutionProvider" in providers:
                print("ðŸ§ª GPUåŠ é€Ÿãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
                try:
                    # ã¾ãšGPUå°‚ç”¨ã§ãƒ†ã‚¹ãƒˆ
                    gpu_session = ort.InferenceSession(model_bytes, providers=['CUDAExecutionProvider'])
                    input_data = np.array([[1.0, 2.0, 3.0]], dtype=np.float32)
                    gpu_result = gpu_session.run(None, {'input': input_data})
                    print(f"âœ… GPUåŠ é€Ÿãƒ†ã‚¹ãƒˆæˆåŠŸ: {gpu_result[0]}")
                    test_success = True
                except Exception as gpu_error:
                    print(f"âš ï¸ GPUåŠ é€Ÿãƒ†ã‚¹ãƒˆå¤±æ•—: {gpu_error}")
                    print("ðŸ”„ CPUãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
            
            if not test_success:
                print("ðŸ§ª CPUå‡¦ç†ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­...")
                try:
                    # CPUã§ã®ãƒ†ã‚¹ãƒˆ
                    cpu_session = ort.InferenceSession(model_bytes, providers=['CPUExecutionProvider'])
                    input_data = np.array([[1.0, 2.0, 3.0]], dtype=np.float32)
                    cpu_result = cpu_session.run(None, {'input': input_data})
                    used_providers = cpu_session.get_providers()
                    print(f"âœ… CPUå‡¦ç†ãƒ†ã‚¹ãƒˆæˆåŠŸ (ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {used_providers[0]})")
                    print(f"âœ… ãƒ†ã‚¹ãƒˆçµæžœ: {cpu_result[0]}")
                    test_success = True
                except Exception as cpu_error:
                    print(f"âŒ CPUå‡¦ç†ãƒ†ã‚¹ãƒˆã‚‚å¤±æ•—: {cpu_error}")
            
            if not test_success:
                raise Exception("Both GPU and CPU tests failed")
                
        except Exception as e:
            print(f"âš ï¸ ONNXãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆåŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯æˆåŠŸï¼‰: {e}")
            print("âœ… ONNXRuntimeåŸºæœ¬ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
            print("ðŸ’¡ å®Ÿéš›ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯å‹•ä½œã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        
        # 8. Visual C++ Redistributableã®ç¢ºèª
        print("ðŸ” Visual C++ Redistributableã®ç¢ºèª...")
        try:
            import ctypes
            kernel32 = ctypes.windll.kernel32
            print("âœ… Windows API ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½")
        except Exception as e:
            print(f"âš ï¸ Windows API ã‚¢ã‚¯ã‚»ã‚¹ã«å•é¡Œ: {e}")
        
        print("âœ… ONNXRuntime exeåŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
        return True
        
    except Exception as e:
        print(f"âŒ ONNXRuntime ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return False

def install_vcredist():
    """Visual C++ Redistributableã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«çŠ¶æ³ã‚’ç¢ºèª"""
    print("ðŸ” Visual C++ Redistributableç¢ºèªä¸­...")
    
    # ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã®VC++ Redistributableã‚’ç¢ºèª
    try:
        import winreg
        
        # 64-bitç‰ˆã®ç¢ºèª
        key_path = r"SOFTWARE\Microsoft\VisualStudio\14.0\VC\Runtimes\x64"
        try:
            with winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, key_path) as key:
                installed, _ = winreg.QueryValueEx(key, "Installed")
                version, _ = winreg.QueryValueEx(key, "Version")
                print(f"âœ… VC++ Redistributable x64 installed: {installed}, version: {version}")
        except FileNotFoundError:
            print("âš ï¸ VC++ Redistributable x64 not found in registry")
        
        # 32-bitç‰ˆã®ç¢ºèª
        key_path = r"SOFTWARE\Microsoft\VisualStudio\14.0\VC\Runtimes\x86"
        try:
            with winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, key_path) as key:
                installed, _ = winreg.QueryValueEx(key, "Installed")
                version, _ = winreg.QueryValueEx(key, "Version")
                print(f"âœ… VC++ Redistributable x86 installed: {installed}, version: {version}")
        except FileNotFoundError:
            print("âš ï¸ VC++ Redistributable x86 not found in registry")
            
    except ImportError:
        print("âš ï¸ winreg module not available")
    except Exception as e:
        print(f"âš ï¸ Registry check failed: {e}")

def create_onnx_test_script():
    """ONNXRuntimeãƒ†ã‚¹ãƒˆç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ"""
    test_script = """
import os
import sys

# ç’°å¢ƒå¤‰æ•°è¨­å®š
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['ORT_LOGGING_LEVEL'] = '3'

print("ðŸ§ª ONNXRuntime exeç’°å¢ƒãƒ†ã‚¹ãƒˆé–‹å§‹...")
print(f"Python version: {sys.version}")
print(f"Platform: {sys.platform}")

try:
    import numpy as np
    print(f"âœ… NumPy version: {np.__version__}")
    
    import onnx
    print(f"âœ… ONNX version: {onnx.__version__}")
    
    import onnxruntime as ort
    print(f"âœ… ONNXRuntime import successful: {ort.__version__}")
    
    providers = ort.get_available_providers()
    print(f"âœ… Available providers: {providers}")
    
    # NudeNetãƒ†ã‚¹ãƒˆ
    try:
        from nudenet import NudeDetector
        print("âœ… NudeDetector import successful")
        
        detector = NudeDetector()
        print("âœ… NudeDetector initialization successful")
        
        # ç°¡å˜ãªæ¤œå‡ºãƒ†ã‚¹ãƒˆ
        from PIL import Image
        test_image = Image.new('RGB', (224, 224), color='white')
        result = detector.detect(test_image)
        print(f"âœ… NudeNet detection test successful: {len(result)} detections")
        
    except Exception as e:
        print(f"âš ï¸ NudeNet test failed: {e}")
    
    print("âœ… ONNXRuntime exeç’°å¢ƒãƒ†ã‚¹ãƒˆå®Œäº†")
    
except Exception as e:
    print(f"âŒ ONNXRuntime test failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
"""
    
    with open("test_onnx_exe.py", "w", encoding="utf-8") as f:
        f.write(test_script)
    
    print("âœ… ONNXRuntimeãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆå®Œäº†: test_onnx_exe.py")

if __name__ == "__main__":
    print("=" * 60)
    print("ONNXRuntime Setup for PyInstaller")
    print("=" * 60)
    
    # Visual C++ Redistributableã®ç¢ºèª
    install_vcredist()
    
    # ONNXRuntimeã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    success = setup_onnxruntime_for_exe()
    
    # ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
    create_onnx_test_script()
    
    if success:
        print("\nâœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†ï¼æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. build_lightweight_dll.spec ã‚’ä½¿ç”¨ã—ã¦ãƒ“ãƒ«ãƒ‰å®Ÿè¡Œ")
        print("2. exeåŒ–å¾Œã«test_onnx_exe.pyã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
        print("\nðŸ“‹ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³:")
        print("   - NumPy: 2.3.0 (æœ€æ–°äº’æ›)")
        print("   - ONNX: 1.16.0 (IR version 10)")
        print("   - ONNXRuntime: 1.22.0 (æœ€æ–°å®‰å®šç‰ˆ)")
        sys.exit(0)
    else:
        print("\nâŒ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        sys.exit(1) 